filebeat.inputs:
  - type: filestream
    id: keywordByMember-logs-filestream
    paths:
      - /usr/share/filebeat/logs/keywordByMember/*.log
  - type: filestream
    id: keyword-logs-filestream
    paths:
      - /usr/share/filebeat/logs/keyword/*.log
  - type: filestream
    id: status-logs-filestream
    paths:
      - /usr/share/filebeat/logs/status/*.log

# 도커 로그 수집을 위해  - /var/run/docker.sock:/var/run/docker.sock 볼륨 설정이 필수.
filebeat.autodiscover:
  providers:
    - type: docker
      hints.enabled: true  # Docker 컨테이너에서 자동으로 로그 수집을 활성화하기 위해 힌트 기반 발견을 사용합니다.
      templates:
        - condition:
            equals:
              docker.container.labels.co.elastic.logs/enabled: "true"
# 위 설정 후,
#     labels:
#      co.elastic.logs/enabled: "true"  # 이 컨테이너의 로그 수집 활성화
#      co.elastic.logs/module: "java"  # Java 모듈을 사용해 로그 처리 (nginx, redis) 등 여러 방식
# 와 같은 것을 도커 컴포즈에는 위 같이, 타 방식은 다른 방식으로 설정해 주면 자동으로 가져옴
processors:
  - add_docker_metadata: ~  # Docker 관련 메타데이터를 수집 이벤트에 자동으로 추가합니다.

setup.kibana:
  host: ${KIBANA_HOSTS}  # Kibana 호스트 주소를 환경 변수에서 가져옵니다.
  #  Kibana에 접속할 때 사용할 사용자 이름과 비밀번호를 설정합니다.
  #  보통 보안이 활성화된 환경에서 사용합니다.
  username: ${ELASTIC_USER}
  password: ${ELASTIC_PASSWORD}

output.elasticsearch:
  # 내가 임의로 추가
  enabled: true  # Elasticsearch 출력을 활성화합니다. 이 옵션은 일반적으로 필요하지 않으며, 이 설정은 표준 구성 옵션이 아닙니다.
  hosts: ${ELASTIC_HOSTS}  # Elasticsearch 호스트 주소를 환경 변수에서 가져옵니다.
  #  아이디 패스워드 설정시에만 하는거 같음
  username: ${ELASTIC_USER}
  password: ${ELASTIC_PASSWORD}
  # ssl 설정 시에만 하는거 같음 (보안상 매우 권장)
  ssl.enabled: true # 이 설정은 SSL을 사용하여 서버와의 통신을 암호화하겠다는 의미입니다. true로 설정하면, Filebeat는 SSL을 통한 보안 연결을 시도합니다.
  ssl.certificate_authorities: "certs/ca/ca.crt" # 이 경로는 Filebeat가 서버의 SSL 인증서를 신뢰하기 위해 참조하는 인증 기관(CA) 인증서의 위치를 지정합니다. 서버의 인증서가 이 CA에 의해 서명된 경우에만 연결이 허용됩니다.

#
# 이 설정은 Docker 컨테이너에서 발생하는 로그와 특정 경로(ingest_data/*.log)에 위치한 로그 파일을 동시에 수집하여 Elasticsearch로 전송합니다.
# 수집된 로그에는 Docker 메타데이터가 추가되며, Kibana를 통해 로그 데이터를 시각화할 수 있습니다.
# 또한, 이 설정은 환경 변수에서 Elasticsearch 및 Kibana의 주소를 가져와 사용하기 때문에, 유연하게 환경 설정을 변경할 수 있습니다.
#

#위의 예시에서는 현재 로그를 전송하려는 서버의 환경 정보와 프로젝트 명을 추가하여 전달하도록 하였다.
#그 결과 아래와 같이 fields.env와 fields.project 필드가 추가된 것을 확인할 수 있다.

#  위의 로그에서 v2, restUri... 로 시작하는 부분은 위의 2021-11-16 ... RestApiBO와 하나의 로그로 보이는데, 로그 출력 시에 \n으로 개행 처리가 되어 있어 별도의 로그로 전송된 것으로 보인다.
#  우리는 2021-11-16 10:59:07 [ 과 같은 패턴을 구분자로 하여 여러 라인의 메세지를 1개의 메세지로 인식하도록 처리해야 한다. 이에 대한 pattern을 분석해보니 다음과 같다.
#
#  숫자4개-숫자2개-숫자2개 숫자2개:숫자2개:숫자2개 [
#
#  그래서 이러한 패턴을 multiline.pattern으로 추가하고, multiline.negate는 true, multiline.match는 after로 주면 되며 이는 같은 패턴이 나올때까지 이전 로그에 붙인다는 것을 의미한다.

# ---

#filebeat.inputs:
#  - type: log
#    enabled: true  # 이 입력을 활성화합니다. false일 경우 이 입력 설정은 무시됩니다.
#
#    paths:
#      # 로그 파일이 위치한 경로를 지정합니다. 예를 들어 /home/mangkyu/logs/tomcat/mangkyu-elk/*.log 같이 설정할 수 있습니다.
#      - /home/로그경로...
#
#    # 필요한 경우 추가적인 필드를 정의할 수 있습니다. 이 필드들은 Elasticsearch에 저장될 때 각 로그에 추가됩니다.
#    fields:
#      env: "real"  # 환경 설정값, 예를 들어 'real'은 실제 운영 환경을 의미합니다.
#      project: "openapi"  # 프로젝트 이름을 지정합니다.
#
#    # 여러 줄로 구성된 로그를 하나의 이벤트로 처리하기 위한 설정입니다.
#    multiline.pattern: ^[0-9]{4}-[0-9]{2}-[0-9]{2}[[:space:]][0-9]{2}:[0-9]{2}:[0-9]{2}[[:space:]]\[
#    multiline.negate: true  # 패턴이 일치하지 않는 행에서 새로운 로그 이벤트가 시작됩니다.
#    multiline.match: after  # 일치하는 패턴 다음 줄부터 현재 이벤트에 포함시킵니다.
#
#setup.kibana:
#  # Kibana 서버의 주소와 포트를 설정합니다. 여기서는 5601 포트를 사용합니다.
#  host: "서버주소:5601"
#
#output.logstash:
#  enabled: true  # Logstash 출력을 활성화합니다. false일 경우 Logstash로의 출력이 비활성화됩니다.
#  # Logstash 서버의 주소와 포트를 설정합니다. 보통 Logstash의 기본 포트는 5000입니다.
#  hosts: ["서버주소:5000"]



# Docker 모듈도 있으며, 이를 활성화하여 사용하면 됩니다:
#filebeat.modules:
#  - module: docker
#    log:
#      input:
#        enabled: true
#        paths:
#          - /var/lib/docker/containers/*/*.log
